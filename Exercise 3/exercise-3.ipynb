{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# geo0930: Insert time series into PostgreSQL/PostGIS and join it with the station info geodata.\n",
    "\n",
    "The main idea behind this activity is to reformat and merge time series (here we use hourly precipitation) as well as weather station information from the DWD Climate Data Center in such a way that it can be used with the **QGIS TimeManager extension**. But this time the **join** of station info geodata and time series are performed in **PostgreSQL/PostGIS** instead of Pandas and CSV file.\n",
    "\n",
    "**UPDATE: The QGIS TimeManager extension is deprecated!** Nevertheless the principle of merging (joining) static location and time varying weather data (temperature, precipitation, etc.) is the same for the **new way to handle time dependent geodata in QGIS.**\n",
    "\n",
    "Below you find the description of how to manage time dependent data with the deprecated TimeManager. This can be transferred to the new QGIS time handling.\n",
    "\n",
    "The TimeManager allows to filter an attribute table of a vector layer (e.g. points representing precipitation stations plus precipitation data) with a time stamp column. The extension limits the attribute table to the records matching the particular time stamp provided by the time manager extension (e.g. by the user moving the time slider). This selected subset of the attribute table is then used to change the sympology of the vector layer according to the variable of interest (e.g. precipitation rate).\n",
    "\n",
    "This relation created by joining station info geodata with time series is a 1:N relationship: 1 station has N measurements values. They can be distinguished by timestamp. Technically the primary key for that relation consists of the two attributes (station_id, timestamp). \n",
    "\n",
    "The final data format is a concatenation of time series together with geographic location in 2D (e.g. lat, lon). The required data format looks principly like this:\n",
    "\n",
    "\n",
    "| station_id |        name        |   lat   |   lon  |        meas_time       | prec_rate |\n",
    "|:----------:|:------------------:|:-------:|:------:|:----------------------:|:---------:|\n",
    "|        ... | ...                |     ... |    ... |                    ... |       ... |\n",
    "|       1595 | Gelsenkirchen-Buer | 51.5762 | 7.0652 | 2018-12-07T08:00:00UTC |       1.5 |\n",
    "|       1595 | Gelsenkirchen-Buer | 51.5762 | 7.0652 | 2018-12-07T09:00:00UTC |       1.7 |\n",
    "|       1595 | Gelsenkirchen-Buer | 51.5762 | 7.0652 | 2018-12-07T10:00:00UTC |       0.1 |\n",
    "|        ... | ...                |     ... |    ... |                    ... |       ... |\n",
    "|      13670 | Duisburg-Baerl     | 51.5088 | 6.7018 | 2018-12-07T08:00:00UTC |       0.8 |\n",
    "|      13670 | Duisburg-Baerl     | 51.5088 | 6.7018 | 2018-12-07T09:00:00UTC |       0.4 |\n",
    "|      13670 | Duisburg-Baerl     | 51.5088 | 6.7018 | 2018-12-07T10:00:00UTC |       0.0 |\n",
    "|        ... | ...                |     ... |    ... |                    ... |       ... |\n",
    "\n",
    "Primary key of this example relation is (station_id, meas_time).\n",
    "\n",
    "(Table generated with https://www.tablesgenerator.com/markdown_tables)\n",
    "\n",
    "This relation was realized in an earlier activity in Pandas and saved as CSV which then was imported to QGIS and used in the TimeManager. This approach is quite brute force, because the data is highly redundent. Example: If the time series at a single station X contains 1000 values then the feature table will contain 1000 rows for that station, one feature with geometry information and measurement value for each timestamp of the time series. Neither station id, station name nor coordinates differ. The only difference are the timestamps and the associated measurement values. And all these 1000 features belonging to one station are plotted on top of each other. The TimeManager then selects from the feature table only those features which match a given timestamp. In this selection each station occurs only once. This view is a snapshot of the precipitation measurements at all stations included for a given time.\n",
    "\n",
    "This activity demonstrates an alternative approach. Instead of writing the 1:N relationship to a CSV file (which can become very large!) and importing this to QGIS the join is performed in PostGIS. The two relations (tables) involved are the station info layer with geometry column (primary key: station_id) and the table with the precipitation time series (Promary key: station_id, timestamp). The join of these tables is then stored as a view. This is a kind of virtual table. When you select from the view it looks as it where a table (in fact, it is a relation), but the information is selected and joined from the underlying tables during execution time.\n",
    "\n",
    "This stored view can be imported in QGIS as point vector layer as if it were a geodata table. It is noteworthy that this link is live connection. Any change of the data in PostGIS will be immediately updated in QGIS and vice versa!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FTP Connection\n",
    "\n",
    "* FTP: ftp://opendata.dwd.de/climate_environment/CDC/observations_germany/\n",
    "* HTTPS: https://opendata.dwd.de/climate_environment/CDC/observations_germany/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = \"opendata.dwd.de\"\n",
    "user   = \"anonymous\"\n",
    "passwd = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FTP Directory Definition and Station Description Filename Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The topic of interest.\n",
    "topic_dir = \"/hourly/precipitation/historical/\"\n",
    "#topic_dir = \"/annual/kl/historical/\"\n",
    "\n",
    "# This is the search pattern common to ALL station description file names \n",
    "station_desc_pattern = \"_Beschreibung_Stationen.txt\"\n",
    "\n",
    "# Below this directory tree node all climate data are stored.\n",
    "ftp_climate_data_dir = \"/climate_environment/CDC/observations_germany/climate/\"\n",
    "ftp_dir =  ftp_climate_data_dir + topic_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and Create Local Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_ftp_dir         = \"../data/original/DWD/\"      # Local directory to store local ftp data copies, the local data source or input data. \n",
    "local_ftp_station_dir = local_ftp_dir + topic_dir # Local directory where local station info is located\n",
    "local_ftp_ts_dir      = local_ftp_dir + topic_dir # Local directory where time series downloaded from ftp are located\n",
    "\n",
    "local_generated_dir   = \"../data/generated/DWD/\" # The generated of derived data in contrast to local_ftp_dir\n",
    "local_station_dir     = local_generated_dir + topic_dir # Derived station data, i.e. the CSV file\n",
    "local_ts_merged_dir   = local_generated_dir + topic_dir # Parallelly merged time series, wide data frame with one TS per column\n",
    "local_ts_appended_dir = local_generated_dir + topic_dir # Serially appended time series, long data frame for QGIS TimeManager Plugin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/original/DWD/\n",
      "../data/original/DWD//hourly/precipitation/historical/\n",
      "../data/original/DWD//hourly/precipitation/historical/\n",
      "\n",
      "../data/generated/DWD/\n",
      "../data/generated/DWD//hourly/precipitation/historical/\n",
      "../data/generated/DWD//hourly/precipitation/historical/\n",
      "../data/generated/DWD//hourly/precipitation/historical/\n"
     ]
    }
   ],
   "source": [
    "print(local_ftp_dir)\n",
    "print(local_ftp_station_dir)\n",
    "print(local_ftp_ts_dir)\n",
    "print()\n",
    "print(local_generated_dir)\n",
    "print(local_station_dir)\n",
    "print(local_ts_merged_dir)\n",
    "print(local_ts_appended_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(local_ftp_dir,exist_ok = True) # it does not complain if the dir already exists.\n",
    "os.makedirs(local_ftp_station_dir,exist_ok = True)\n",
    "os.makedirs(local_ftp_ts_dir,exist_ok = True)\n",
    "\n",
    "os.makedirs(local_generated_dir,exist_ok = True)\n",
    "os.makedirs(local_station_dir,exist_ok = True)\n",
    "os.makedirs(local_ts_merged_dir,exist_ok = True)\n",
    "os.makedirs(local_ts_appended_dir,exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FTP Connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230 Login successful.\n"
     ]
    }
   ],
   "source": [
    "import ftplib\n",
    "ftp = ftplib.FTP(server)\n",
    "res = ftp.login(user=user, passwd = passwd)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = ftp.cwd(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ftp.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Pandas Dataframe from FTP Directory Listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>name</th>\n",
       "      <th>ext</th>\n",
       "      <th>size</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>BESCHREIBUNG_obsgermany_climate_hourly_precipi...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>166317</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>DESCRIPTION_obsgermany_climate_hourly_precipit...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>161348</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>RR_Stundenwerte_Beschreibung_Stationen.txt</td>\n",
       "      <td>.txt</td>\n",
       "      <td>303009</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>stundenwerte_RR_00003_19950901_20110401_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>419296</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>stundenwerte_RR_00020_20040814_20201231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>432124</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_id                                               name   ext  \\\n",
       "0          -1  BESCHREIBUNG_obsgermany_climate_hourly_precipi...  .pdf   \n",
       "1          -1  DESCRIPTION_obsgermany_climate_hourly_precipit...  .pdf   \n",
       "2          -1         RR_Stundenwerte_Beschreibung_Stationen.txt  .txt   \n",
       "3           3   stundenwerte_RR_00003_19950901_20110401_hist.zip  .zip   \n",
       "4          20   stundenwerte_RR_00020_20040814_20201231_hist.zip  .zip   \n",
       "\n",
       "     size type  \n",
       "0  166317    -  \n",
       "1  161348    -  \n",
       "2  303009    -  \n",
       "3  419296    -  \n",
       "4  432124    -  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from my_dwd import gen_df_from_ftp_dir_listing\n",
    "df_ftpdir = gen_df_from_ftp_dir_listing(ftp, ftp_dir)\n",
    "df_ftpdir.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Process the Station Description File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab the txt File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_dwd import grabFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station description file name:\n",
      "RR_Stundenwerte_Beschreibung_Stationen.txt\n"
     ]
    }
   ],
   "source": [
    "station_fname = df_ftpdir[df_ftpdir['name'].str.contains(station_desc_pattern)][\"name\"].values[0]\n",
    "print(\"Station description file name:\\n%s\" % (station_fname))\n",
    "\n",
    "# ALternative\n",
    "#station_fname2 = df_ftpdir[df_ftpdir[\"name\"].str.match(\"^.*Beschreibung_Stationen.*txt$\")][\"name\"].values[0]\n",
    "#print(station_fname2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grabFile(ftp, src, dest):\n",
      "FTP source: /climate_environment/CDC/observations_germany/climate//hourly/precipitation/historical/RR_Stundenwerte_Beschreibung_Stationen.txt\n",
      "Local dest:   ../data/original/DWD//hourly/precipitation/historical/RR_Stundenwerte_Beschreibung_Stationen.txt\n"
     ]
    }
   ],
   "source": [
    "src = ftp_dir + station_fname\n",
    "dest = local_ftp_station_dir + station_fname\n",
    "print(\"grabFile(ftp, src, dest):\")\n",
    "print(\"FTP source: \" + src)\n",
    "print(\"Local dest:   \" + dest)\n",
    "grabFile(ftp, src, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename the Column Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_from</th>\n",
       "      <th>date_to</th>\n",
       "      <th>altitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1995-09-01</td>\n",
       "      <td>2011-04-01</td>\n",
       "      <td>202</td>\n",
       "      <td>50.7827</td>\n",
       "      <td>6.0941</td>\n",
       "      <td>Aachen</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2004-08-14</td>\n",
       "      <td>2022-03-07</td>\n",
       "      <td>432</td>\n",
       "      <td>48.9219</td>\n",
       "      <td>9.9129</td>\n",
       "      <td>Abtsgmünd-Untergröningen</td>\n",
       "      <td>Baden-Württemberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2006-01-10</td>\n",
       "      <td>2022-03-07</td>\n",
       "      <td>260</td>\n",
       "      <td>49.7175</td>\n",
       "      <td>10.9101</td>\n",
       "      <td>Adelsdorf (Kläranlage)</td>\n",
       "      <td>Bayern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2007-04-01</td>\n",
       "      <td>2022-03-07</td>\n",
       "      <td>44</td>\n",
       "      <td>52.9336</td>\n",
       "      <td>8.2370</td>\n",
       "      <td>Großenkneten</td>\n",
       "      <td>Niedersachsen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2006-01-03</td>\n",
       "      <td>2022-03-07</td>\n",
       "      <td>325</td>\n",
       "      <td>48.9450</td>\n",
       "      <td>12.4639</td>\n",
       "      <td>Aholfing</td>\n",
       "      <td>Bayern</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date_from    date_to  altitude  latitude  longitude  \\\n",
       "station_id                                                        \n",
       "3          1995-09-01 2011-04-01       202   50.7827     6.0941   \n",
       "20         2004-08-14 2022-03-07       432   48.9219     9.9129   \n",
       "29         2006-01-10 2022-03-07       260   49.7175    10.9101   \n",
       "44         2007-04-01 2022-03-07        44   52.9336     8.2370   \n",
       "46         2006-01-03 2022-03-07       325   48.9450    12.4639   \n",
       "\n",
       "                                name                state  \n",
       "station_id                                                 \n",
       "3                             Aachen  Nordrhein-Westfalen  \n",
       "20          Abtsgmünd-Untergröningen    Baden-Württemberg  \n",
       "29            Adelsdorf (Kläranlage)               Bayern  \n",
       "44                      Großenkneten        Niedersachsen  \n",
       "46                          Aholfing               Bayern  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from my_dwd import station_desc_txt_to_csv\n",
    "basename = os.path.splitext(station_fname)[0]\n",
    "df_stations = station_desc_txt_to_csv(local_ftp_station_dir + station_fname, local_station_dir + basename + \".csv\")\n",
    "df_stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only Stations Located in NRW and being Operational "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#station_ids_selected = df_stations[df_stations['state'].str.contains(\"Nordrhein\")].index\n",
    "#station_ids_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_from</th>\n",
       "      <th>date_to</th>\n",
       "      <th>altitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>2004-10-01</td>\n",
       "      <td>2022-03-07</td>\n",
       "      <td>298</td>\n",
       "      <td>51.1143</td>\n",
       "      <td>7.8807</td>\n",
       "      <td>Attendorn-Neulisternohl</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>2009-11-01</td>\n",
       "      <td>2022-03-07</td>\n",
       "      <td>436</td>\n",
       "      <td>51.0148</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>Berleburg, Bad-Arfeld</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>2004-07-01</td>\n",
       "      <td>2022-03-07</td>\n",
       "      <td>610</td>\n",
       "      <td>50.9837</td>\n",
       "      <td>8.3683</td>\n",
       "      <td>Berleburg, Bad-Stünzel</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>1995-09-01</td>\n",
       "      <td>2022-03-07</td>\n",
       "      <td>23</td>\n",
       "      <td>51.8293</td>\n",
       "      <td>6.5365</td>\n",
       "      <td>Bocholt-Liedern (Wasserwerk)</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>1999-03-03</td>\n",
       "      <td>2022-03-07</td>\n",
       "      <td>147</td>\n",
       "      <td>50.7293</td>\n",
       "      <td>7.2040</td>\n",
       "      <td>Königswinter-Heiderhof</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14183</th>\n",
       "      <td>2016-06-01</td>\n",
       "      <td>2022-03-07</td>\n",
       "      <td>297</td>\n",
       "      <td>51.1433</td>\n",
       "      <td>7.3656</td>\n",
       "      <td>Hückeswagen/Talsperre</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14184</th>\n",
       "      <td>2016-06-01</td>\n",
       "      <td>2022-03-07</td>\n",
       "      <td>126</td>\n",
       "      <td>51.2242</td>\n",
       "      <td>7.1070</td>\n",
       "      <td>Wuppertal-Buchenhofen/Wupper</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14185</th>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>2022-03-07</td>\n",
       "      <td>304</td>\n",
       "      <td>51.6050</td>\n",
       "      <td>8.8175</td>\n",
       "      <td>Lichtenau-Ebbinghausen (HRB)</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14186</th>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>2022-03-07</td>\n",
       "      <td>291</td>\n",
       "      <td>51.5319</td>\n",
       "      <td>8.7289</td>\n",
       "      <td>Gollentaler Grund (HRB)</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>2011-04-01</td>\n",
       "      <td>2022-03-07</td>\n",
       "      <td>231</td>\n",
       "      <td>50.7983</td>\n",
       "      <td>6.0244</td>\n",
       "      <td>Aachen-Orsbach</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date_from    date_to  altitude  latitude  longitude  \\\n",
       "station_id                                                        \n",
       "216        2004-10-01 2022-03-07       298   51.1143     7.8807   \n",
       "389        2009-11-01 2022-03-07       436   51.0148     8.4318   \n",
       "390        2004-07-01 2022-03-07       610   50.9837     8.3683   \n",
       "554        1995-09-01 2022-03-07        23   51.8293     6.5365   \n",
       "603        1999-03-03 2022-03-07       147   50.7293     7.2040   \n",
       "...               ...        ...       ...       ...        ...   \n",
       "14183      2016-06-01 2022-03-07       297   51.1433     7.3656   \n",
       "14184      2016-06-01 2022-03-07       126   51.2242     7.1070   \n",
       "14185      2017-08-01 2022-03-07       304   51.6050     8.8175   \n",
       "14186      2017-08-01 2022-03-07       291   51.5319     8.7289   \n",
       "15000      2011-04-01 2022-03-07       231   50.7983     6.0244   \n",
       "\n",
       "                                    name                state  \n",
       "station_id                                                     \n",
       "216              Attendorn-Neulisternohl  Nordrhein-Westfalen  \n",
       "389                Berleburg, Bad-Arfeld  Nordrhein-Westfalen  \n",
       "390               Berleburg, Bad-Stünzel  Nordrhein-Westfalen  \n",
       "554         Bocholt-Liedern (Wasserwerk)  Nordrhein-Westfalen  \n",
       "603               Königswinter-Heiderhof  Nordrhein-Westfalen  \n",
       "...                                  ...                  ...  \n",
       "14183              Hückeswagen/Talsperre  Nordrhein-Westfalen  \n",
       "14184       Wuppertal-Buchenhofen/Wupper  Nordrhein-Westfalen  \n",
       "14185       Lichtenau-Ebbinghausen (HRB)  Nordrhein-Westfalen  \n",
       "14186            Gollentaler Grund (HRB)  Nordrhein-Westfalen  \n",
       "15000                     Aachen-Orsbach  Nordrhein-Westfalen  \n",
       "\n",
       "[144 rows x 7 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create variable with TRUE if state is Nordrhein-Westfalen\n",
    "\n",
    "# isNRW = df_stations['state'] == \"Nordrhein-Westfalen\"\n",
    "isNRW = df_stations['state'].str.contains(\"Nordrhein\")\n",
    "\n",
    "# Create variable with TRUE if date_to is latest date (indicates operation up to now)\n",
    "isOperational = df_stations['date_to'] == df_stations.date_to.max() \n",
    "\n",
    "#isBefore1950 = df_stations['date_from'] < '1950'\n",
    "#dfNRW = df_stations[isNRW & isOperational & isBefore1950]\n",
    "\n",
    "# select on both conditions\n",
    "\n",
    "dfNRW = df_stations[isNRW & isOperational]\n",
    "\n",
    "#print(\"Number of stations in NRW: \\n\", dfNRW.count())\n",
    "dfNRW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geopandas - Create a Geo Data Frame\n",
    "\n",
    "A Geopandas geo data frame is a Pandas data frame enriched with an additional geometry column. Each row in the data frame becomes a location information. Thus a geo-df contains geometry and attributes, i.e. full features. The geo-df is self-contained and complete. It can be easily saved in different vectore file formats, i.e. shapefile or geopackage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue: some `pyproj` installations with wrong `PROJ_LIB` environment variable value \n",
    "\n",
    "Problem:\n",
    "\n",
    "```\n",
    "C:\\Users\\me\\Anaconda3\\envs\\geo\\lib\\site-packages\\pyproj\\__init__.py:89: UserWarning: pyproj unable to set database path.\n",
    "  _pyproj_global_context_initialize()\n",
    "[...]\n",
    "CRSError: Invalid projection: epsg:4326: (Internal Proj Error: proj_create: no database context specified)\n",
    "```\n",
    "\n",
    "\n",
    "This problem seems to occur on Windows when using the OSGeo4W installer. The environment variable must point to a user specific directory and according to the activated conda environment, e.g. `PROJ_LIB=C:\\Users\\<username>\\Anaconda3\\envs\\geo\\Library\\share\\proj` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONDA_PREFIX: C:\\Users\\Muhammad Tanvir\\anaconda3\\envs\\geo\n",
      "New env var value: \n",
      "PROJ_LIB=C:\\Users\\Muhammad Tanvir\\anaconda3\\envs\\geo\\Library\\share\\proj\n",
      "pyproj.datadir.get_data_dir() -> C:\\Users\\Muhammad Tanvir\\anaconda3\\envs\\geo\\Library\\share\\proj\n"
     ]
    }
   ],
   "source": [
    "# Correct wrong environment variable value occurring when using OSGeo4W installer\n",
    "\n",
    "import os\n",
    "#proj_lib = os.environ['proj_lib']\n",
    "#print(proj_lib)\n",
    "#-> C:\\OSGeo4W64\\share\\proj (wrong!)\n",
    "\n",
    "conda_prefix = os.environ['conda_prefix']\n",
    "print(f\"CONDA_PREFIX: {conda_prefix:s}\")\n",
    "os.environ['proj_lib'] = conda_prefix + r\"\\Library\\share\\proj\"\n",
    "proj_lib = os.environ['proj_lib']\n",
    "print(f\"New env var value: \\nPROJ_LIB={proj_lib:s}\")\n",
    "#-> C:\\Users\\me\\Anaconda3\\envs\\geo\\Library\\share\\proj (correct!)\n",
    "\n",
    "# Now pyproj should work\n",
    "import pyproj\n",
    "print(f\"pyproj.datadir.get_data_dir() -> {pyproj.datadir.get_data_dir():s}\") \n",
    "\n",
    "# Now geopandas (it uses pyproj) should work:\n",
    "# import geopandas as gpd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_from</th>\n",
       "      <th>date_to</th>\n",
       "      <th>altitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>state</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>2004-10-01</td>\n",
       "      <td>2022-03-07</td>\n",
       "      <td>298</td>\n",
       "      <td>51.1143</td>\n",
       "      <td>7.8807</td>\n",
       "      <td>Attendorn-Neulisternohl</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "      <td>POINT (7.88070 51.11430)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>2009-11-01</td>\n",
       "      <td>2022-03-07</td>\n",
       "      <td>436</td>\n",
       "      <td>51.0148</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>Berleburg, Bad-Arfeld</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "      <td>POINT (8.43180 51.01480)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>2004-07-01</td>\n",
       "      <td>2022-03-07</td>\n",
       "      <td>610</td>\n",
       "      <td>50.9837</td>\n",
       "      <td>8.3683</td>\n",
       "      <td>Berleburg, Bad-Stünzel</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "      <td>POINT (8.36830 50.98370)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>1995-09-01</td>\n",
       "      <td>2022-03-07</td>\n",
       "      <td>23</td>\n",
       "      <td>51.8293</td>\n",
       "      <td>6.5365</td>\n",
       "      <td>Bocholt-Liedern (Wasserwerk)</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "      <td>POINT (6.53650 51.82930)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>1999-03-03</td>\n",
       "      <td>2022-03-07</td>\n",
       "      <td>147</td>\n",
       "      <td>50.7293</td>\n",
       "      <td>7.2040</td>\n",
       "      <td>Königswinter-Heiderhof</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "      <td>POINT (7.20400 50.72930)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date_from    date_to  altitude  latitude  longitude  \\\n",
       "station_id                                                        \n",
       "216        2004-10-01 2022-03-07       298   51.1143     7.8807   \n",
       "389        2009-11-01 2022-03-07       436   51.0148     8.4318   \n",
       "390        2004-07-01 2022-03-07       610   50.9837     8.3683   \n",
       "554        1995-09-01 2022-03-07        23   51.8293     6.5365   \n",
       "603        1999-03-03 2022-03-07       147   50.7293     7.2040   \n",
       "\n",
       "                                    name                state  \\\n",
       "station_id                                                      \n",
       "216              Attendorn-Neulisternohl  Nordrhein-Westfalen   \n",
       "389                Berleburg, Bad-Arfeld  Nordrhein-Westfalen   \n",
       "390               Berleburg, Bad-Stünzel  Nordrhein-Westfalen   \n",
       "554         Bocholt-Liedern (Wasserwerk)  Nordrhein-Westfalen   \n",
       "603               Königswinter-Heiderhof  Nordrhein-Westfalen   \n",
       "\n",
       "                            geometry  \n",
       "station_id                            \n",
       "216         POINT (7.88070 51.11430)  \n",
       "389         POINT (8.43180 51.01480)  \n",
       "390         POINT (8.36830 50.98370)  \n",
       "554         POINT (6.53650 51.82930)  \n",
       "603         POINT (7.20400 50.72930)  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from geopandas import GeoDataFrame\n",
    "from shapely.geometry import Point\n",
    "import fiona\n",
    "from pyproj import CRS\n",
    "\n",
    "#df = pd.read_csv('data.csv')\n",
    "df = dfNRW\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(df.longitude, df.latitude)]\n",
    "crs = CRS(\"epsg:4326\") #http://www.spatialreference.org/ref/epsg/2263/\n",
    "stations_gdf = GeoDataFrame(df, crs=crs, geometry=geometry)\n",
    "\n",
    "stations_gdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the PostGIS database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection URL:  postgresql://geo_master:xxxxxx@localhost:5432/geo\n"
     ]
    }
   ],
   "source": [
    "# PostgreSQL connection parameters -> create connection string (URL) \n",
    "\n",
    "param_dic = {\n",
    "  \"user\" : \"geo_master\",\n",
    "  \"pw\"   : \"xxxxxx\",\n",
    "  \"host\" : \"localhost\",\n",
    "  \"db\"   : \"geo\"\n",
    "}\n",
    "\n",
    "# https://www.w3schools.com/python/ref_string_format.asp\n",
    "template = \"postgresql://{user}:{pw}@{host}:5432/{db}\"\n",
    "\n",
    "db_connection_url = template.format(**param_dic)\n",
    "print(\"Connection URL: \", db_connection_url) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Geopandas Data Frame directly into PostGIS Database\n",
    "\n",
    "* https://geopandas.readthedocs.io/en/latest/docs/reference/api/geopandas.GeoDataFrame.to_postgis.html\n",
    "* https://docs.sqlalchemy.org/en/13/core/types.html\n",
    "* https://www.postgresqltutorial.com/postgresql-primary-key/\n",
    "* https://www.postgresql.org/docs/13/sql-altertable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://geopandas.readthedocs.io/en/latest/docs/reference/api/geopandas.GeoDataFrame.to_postgis.html\n",
    "# https://docs.sqlalchemy.org/en/13/core/types.html\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import Numeric, Float, Date, REAL\n",
    "#import psycopg2\n",
    "\n",
    "engine = create_engine(db_connection_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.LegacyCursorResult at 0x1a7cb5915e0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set data types in PG explicitly.\n",
    "dtypes = {\"station_id\": Numeric(6,0), \"altitude\" : REAL, \"date_from\" : Date, \"date_to\" : Date, \"longitude\" : REAL, \"latitude\" : REAL}\n",
    "\n",
    "# to_postgis() option if_exists = \"replace\" does not imply cascade. The table cannot be replaced if other objects depend on it, i.e. the view used later. \n",
    "# This raises an error.\n",
    "# you may have to drop the table explicitly with option cascade or drop the view.\n",
    "# ATTENTION! This drops all dependent objects, too, i.e. the view!\n",
    "#engine.execute('drop table dwd.stations cascade') \n",
    "# Just drop the view ...\n",
    "engine.execute(\"DROP VIEW IF EXISTS dwd.v_stations_prec\")\n",
    "\n",
    "stations_gdf.to_postgis(name=\"stations\", schema=\"dwd\", if_exists = \"replace\", index = \"station_id\", index_label=True, con=engine, dtype=dtypes)\n",
    "\n",
    "#engine.execute('alter table dwd.stations add constraint my_awesome_pkey primary key (station_id)')\n",
    "engine.execute('alter table dwd.stations add primary key (station_id)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Process the Time Series Zip Archives\n",
    "\n",
    "Extract the product file (txt file containing several time series for different variables) from an archive, extract the relevant time series from the product file, limit the time series interval if needed and append it to a dataframe. Finally insert the dataframe to the PostGIS database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe with TS Zip Files from FTP Directory Listing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>ext</th>\n",
       "      <th>size</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stundenwerte_RR_00003_19950901_20110401_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>419296</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>stundenwerte_RR_00020_20040814_20201231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>432124</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>stundenwerte_RR_00044_20070401_20201231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>354983</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>stundenwerte_RR_00053_20051001_20201231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>385830</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>stundenwerte_RR_00071_20041022_20200101_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>402875</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        name   ext    size  \\\n",
       "station_id                                                                   \n",
       "3           stundenwerte_RR_00003_19950901_20110401_hist.zip  .zip  419296   \n",
       "20          stundenwerte_RR_00020_20040814_20201231_hist.zip  .zip  432124   \n",
       "44          stundenwerte_RR_00044_20070401_20201231_hist.zip  .zip  354983   \n",
       "53          stundenwerte_RR_00053_20051001_20201231_hist.zip  .zip  385830   \n",
       "71          stundenwerte_RR_00071_20041022_20200101_hist.zip  .zip  402875   \n",
       "\n",
       "           type  \n",
       "station_id       \n",
       "3             -  \n",
       "20            -  \n",
       "44            -  \n",
       "53            -  \n",
       "71            -  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_ftpdir[\"ext\"]==\".zip\"\n",
    "df_zips = df_ftpdir[df_ftpdir[\"ext\"]==\".zip\"]\n",
    "df_zips.set_index(\"station_id\", inplace = True)\n",
    "df_zips.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download TS Data from FTP Server\n",
    "\n",
    "**Problem:** Not all stations listed in the station description file are associated with a time series (zip file)! The stations in the description file and the set of stations whoch are TS data provided for (zip files) do not match perfectly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stundenwerte_RR_00216_20041001_20201231_hist.zip\n",
      "stundenwerte_RR_00389_20091101_20201231_hist.zip\n",
      "stundenwerte_RR_00390_20040701_20201231_hist.zip\n",
      "stundenwerte_RR_00554_19950901_20201231_hist.zip\n",
      "stundenwerte_RR_00603_19990303_20201231_hist.zip\n",
      "stundenwerte_RR_00613_20041101_20201231_hist.zip\n",
      "stundenwerte_RR_00617_20040601_20201231_hist.zip\n",
      "stundenwerte_RR_00644_20050101_20201231_hist.zip\n",
      "stundenwerte_RR_00796_20041101_20201231_hist.zip\n",
      "stundenwerte_RR_00871_20050801_20201231_hist.zip\n",
      "stundenwerte_RR_00902_20061001_20201231_hist.zip\n",
      "stundenwerte_RR_00934_20041001_20201231_hist.zip\n",
      "stundenwerte_RR_00989_20050201_20201231_hist.zip\n",
      "stundenwerte_RR_01024_20060801_20201231_hist.zip\n",
      "stundenwerte_RR_01046_20041001_20201231_hist.zip\n",
      "stundenwerte_RR_01078_19950901_20201231_hist.zip\n",
      "stundenwerte_RR_01241_20061201_20201231_hist.zip\n",
      "stundenwerte_RR_01246_20150801_20201231_hist.zip\n",
      "stundenwerte_RR_01300_20040601_20201231_hist.zip\n",
      "stundenwerte_RR_01303_19950901_20201231_hist.zip\n",
      "stundenwerte_RR_01327_20040801_20201231_hist.zip\n",
      "stundenwerte_RR_01590_20030701_20201231_hist.zip\n",
      "stundenwerte_RR_01595_20121001_20201231_hist.zip\n",
      "stundenwerte_RR_01766_19950901_20201231_hist.zip\n",
      "stundenwerte_RR_02027_20060601_20201231_hist.zip\n",
      "stundenwerte_RR_02110_20030105_20201231_hist.zip\n",
      "stundenwerte_RR_02254_20050601_20201231_hist.zip\n",
      "stundenwerte_RR_02473_20041201_20201231_hist.zip\n",
      "stundenwerte_RR_02483_19951012_20201231_hist.zip\n",
      "stundenwerte_RR_02497_20040801_20201231_hist.zip\n",
      "stundenwerte_RR_02629_20040701_20201231_hist.zip\n",
      "stundenwerte_RR_02667_19950901_20201231_hist.zip\n",
      "stundenwerte_RR_02810_20061201_20201231_hist.zip\n",
      "stundenwerte_RR_02947_20061001_20201231_hist.zip\n",
      "stundenwerte_RR_02968_20081201_20201231_hist.zip\n",
      "stundenwerte_RR_02999_20040701_20201231_hist.zip\n",
      "stundenwerte_RR_03028_19950901_20201231_hist.zip\n",
      "stundenwerte_RR_03031_20040701_20201231_hist.zip\n",
      "stundenwerte_RR_03081_20071201_20201231_hist.zip\n",
      "stundenwerte_RR_03098_19950901_20201231_hist.zip\n",
      "stundenwerte_RR_03215_20070601_20201231_hist.zip\n",
      "stundenwerte_RR_03321_20050701_20201231_hist.zip\n",
      "WARNING: TS file for key 3328 not found in FTP directory.\n",
      "stundenwerte_RR_03339_20060901_20201231_hist.zip\n",
      "stundenwerte_RR_03499_20060701_20201231_hist.zip\n",
      "stundenwerte_RR_03540_20041101_20201231_hist.zip\n",
      "stundenwerte_RR_03591_20040601_20201231_hist.zip\n",
      "stundenwerte_RR_03795_20041201_20201231_hist.zip\n",
      "stundenwerte_RR_03913_20040701_20201231_hist.zip\n",
      "stundenwerte_RR_04063_20030701_20201231_hist.zip\n",
      "stundenwerte_RR_04127_20050101_20201231_hist.zip\n",
      "stundenwerte_RR_04150_20051201_20201231_hist.zip\n",
      "stundenwerte_RR_04313_20040801_20201231_hist.zip\n",
      "stundenwerte_RR_04368_20041001_20201231_hist.zip\n",
      "stundenwerte_RR_04371_19950901_20201231_hist.zip\n",
      "stundenwerte_RR_04400_20040801_20201231_hist.zip\n",
      "stundenwerte_RR_04488_20060801_20201231_hist.zip\n",
      "stundenwerte_RR_04741_20041201_20201231_hist.zip\n",
      "stundenwerte_RR_04849_20050601_20201231_hist.zip\n",
      "stundenwerte_RR_05064_20041201_20201231_hist.zip\n",
      "stundenwerte_RR_05347_19950901_20201231_hist.zip\n",
      "stundenwerte_RR_05360_20070701_20201231_hist.zip\n",
      "stundenwerte_RR_05480_20030910_20201231_hist.zip\n",
      "stundenwerte_RR_05513_20050901_20201231_hist.zip\n",
      "stundenwerte_RR_05619_20041201_20201231_hist.zip\n",
      "stundenwerte_RR_05699_20041101_20201231_hist.zip\n",
      "stundenwerte_RR_05717_20060901_20201231_hist.zip\n",
      "stundenwerte_RR_05733_20050501_20201231_hist.zip\n",
      "WARNING: TS file for key 6041 not found in FTP directory.\n",
      "WARNING: TS file for key 6042 not found in FTP directory.\n",
      "WARNING: TS file for key 6043 not found in FTP directory.\n",
      "WARNING: TS file for key 6044 not found in FTP directory.\n",
      "WARNING: TS file for key 6045 not found in FTP directory.\n",
      "WARNING: TS file for key 6046 not found in FTP directory.\n",
      "WARNING: TS file for key 6047 not found in FTP directory.\n",
      "WARNING: TS file for key 6048 not found in FTP directory.\n",
      "WARNING: TS file for key 6050 not found in FTP directory.\n",
      "WARNING: TS file for key 6051 not found in FTP directory.\n",
      "WARNING: TS file for key 6052 not found in FTP directory.\n",
      "WARNING: TS file for key 6053 not found in FTP directory.\n",
      "WARNING: TS file for key 6054 not found in FTP directory.\n",
      "WARNING: TS file for key 6055 not found in FTP directory.\n",
      "WARNING: TS file for key 6057 not found in FTP directory.\n",
      "WARNING: TS file for key 6058 not found in FTP directory.\n",
      "WARNING: TS file for key 6059 not found in FTP directory.\n",
      "WARNING: TS file for key 6060 not found in FTP directory.\n",
      "WARNING: TS file for key 6061 not found in FTP directory.\n",
      "WARNING: TS file for key 6064 not found in FTP directory.\n",
      "WARNING: TS file for key 6067 not found in FTP directory.\n",
      "stundenwerte_RR_06197_20001013_20201231_hist.zip\n",
      "stundenwerte_RR_06264_20040601_20201231_hist.zip\n",
      "stundenwerte_RR_06313_20041201_20201231_hist.zip\n",
      "stundenwerte_RR_06337_20040801_20201231_hist.zip\n",
      "stundenwerte_RR_07106_20060901_20201231_hist.zip\n",
      "stundenwerte_RR_07330_20051001_20201231_hist.zip\n",
      "stundenwerte_RR_07344_20060601_20201231_hist.zip\n",
      "stundenwerte_RR_07374_20060301_20201231_hist.zip\n",
      "stundenwerte_RR_07378_20060701_20201231_hist.zip\n",
      "stundenwerte_RR_13669_20070901_20201231_hist.zip\n",
      "stundenwerte_RR_13670_20070601_20201231_hist.zip\n",
      "stundenwerte_RR_13671_20071201_20201231_hist.zip\n",
      "stundenwerte_RR_13696_20071201_20201231_hist.zip\n",
      "stundenwerte_RR_13700_20080501_20201231_hist.zip\n",
      "stundenwerte_RR_13713_20071101_20201231_hist.zip\n",
      "WARNING: TS file for key 14096 not found in FTP directory.\n",
      "WARNING: TS file for key 14142 not found in FTP directory.\n",
      "WARNING: TS file for key 14143 not found in FTP directory.\n",
      "WARNING: TS file for key 14144 not found in FTP directory.\n",
      "WARNING: TS file for key 14145 not found in FTP directory.\n",
      "WARNING: TS file for key 14146 not found in FTP directory.\n",
      "WARNING: TS file for key 14147 not found in FTP directory.\n",
      "WARNING: TS file for key 14148 not found in FTP directory.\n",
      "WARNING: TS file for key 14149 not found in FTP directory.\n",
      "WARNING: TS file for key 14150 not found in FTP directory.\n",
      "WARNING: TS file for key 14151 not found in FTP directory.\n",
      "WARNING: TS file for key 14152 not found in FTP directory.\n",
      "WARNING: TS file for key 14153 not found in FTP directory.\n",
      "WARNING: TS file for key 14154 not found in FTP directory.\n",
      "WARNING: TS file for key 14155 not found in FTP directory.\n",
      "WARNING: TS file for key 14156 not found in FTP directory.\n",
      "WARNING: TS file for key 14158 not found in FTP directory.\n",
      "WARNING: TS file for key 14159 not found in FTP directory.\n",
      "WARNING: TS file for key 14164 not found in FTP directory.\n",
      "WARNING: TS file for key 14165 not found in FTP directory.\n",
      "WARNING: TS file for key 14166 not found in FTP directory.\n",
      "WARNING: TS file for key 14167 not found in FTP directory.\n",
      "WARNING: TS file for key 14168 not found in FTP directory.\n",
      "WARNING: TS file for key 14169 not found in FTP directory.\n",
      "WARNING: TS file for key 14170 not found in FTP directory.\n",
      "WARNING: TS file for key 14171 not found in FTP directory.\n",
      "WARNING: TS file for key 14172 not found in FTP directory.\n",
      "WARNING: TS file for key 14173 not found in FTP directory.\n",
      "WARNING: TS file for key 14174 not found in FTP directory.\n",
      "WARNING: TS file for key 14176 not found in FTP directory.\n",
      "WARNING: TS file for key 14177 not found in FTP directory.\n",
      "WARNING: TS file for key 14178 not found in FTP directory.\n",
      "WARNING: TS file for key 14179 not found in FTP directory.\n",
      "WARNING: TS file for key 14181 not found in FTP directory.\n",
      "WARNING: TS file for key 14182 not found in FTP directory.\n",
      "WARNING: TS file for key 14183 not found in FTP directory.\n",
      "WARNING: TS file for key 14184 not found in FTP directory.\n",
      "WARNING: TS file for key 14185 not found in FTP directory.\n",
      "WARNING: TS file for key 14186 not found in FTP directory.\n",
      "stundenwerte_RR_15000_20110401_20201231_hist.zip\n"
     ]
    }
   ],
   "source": [
    "# Add the names of the actually downloaded zip files to this list. \n",
    "local_zip_list = []\n",
    "\n",
    "# SHORTENED FOR TESTING!\n",
    "#station_ids_selected = list(dfNRW.index)[:2]\n",
    "station_ids_selected = list(dfNRW.index)\n",
    "\n",
    "for station_id in station_ids_selected:\n",
    "    try:\n",
    "        fname = df_zips[\"name\"][station_id]\n",
    "        print(fname)\n",
    "        grabFile(ftp, ftp_dir + fname, local_ftp_ts_dir + fname)\n",
    "        local_zip_list.append(fname)\n",
    "    except:\n",
    "        print(\"WARNING: TS file for key %d not found in FTP directory.\" % station_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local_zip_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Write the time series to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "from my_dwd import prec_ts_to_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following code is not necessary! It just produces a large CSV file in your folder for testing!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CODE EXECUTION DEACTIVATED! Change it to True if you want to run it.\n",
    "#if True:\n",
    "if False:\n",
    "    # Produce CSV with sequentially appended time series, ca. 120 MB!\n",
    "    csvfname = \"prec_ts_appended_3_cols.csv\"\n",
    "    first = False\n",
    "    for elt in local_zip_list:\n",
    "        ffname = local_ftp_ts_dir + elt\n",
    "        print(\"Zip archive: \" + ffname)\n",
    "        with ZipFile(ffname) as myzip:\n",
    "            # read the time series data from the file starting with \"produkt\"\n",
    "            prodfilename = [elt for elt in myzip.namelist() if elt.split(\"_\")[0]==\"produkt\"][0] \n",
    "            print(\"Extract product file: %s\" % prodfilename)\n",
    "            print()\n",
    "            with myzip.open(prodfilename) as myfile:\n",
    "                dftmp = prec_ts_to_df(myfile)[[\"stations_id\",\"r1\"]]\n",
    "                # df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)\n",
    "                dftmp.rename(columns={'stations_id': 'station_id', 'r1': 'val', 'mess_datum': 'ts'}, inplace = True)\n",
    "                dftmp.rename_axis('ts', inplace = True)\n",
    "                # dftmp.to_csv(f, header=f.tell()==0)\n",
    "                if (first):\n",
    "                    first = False\n",
    "                    dftmp.to_csv(csvfname, mode = \"w\", header = True)\n",
    "                else:\n",
    "                    dftmp.to_csv(csvfname, mode = \"a\", header = False)\n",
    "\n",
    "    dftmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The database writer (SQL)! ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract product file: produkt_rr_stunde_20041001_20201231_00216.txt\n",
      "Extract product file: produkt_rr_stunde_20091101_20201231_00389.txt\n",
      "Extract product file: produkt_rr_stunde_20040701_20201231_00390.txt\n",
      "Extract product file: produkt_rr_stunde_19950901_20201231_00554.txt\n",
      "Extract product file: produkt_rr_stunde_19990303_20201231_00603.txt\n",
      "Extract product file: produkt_rr_stunde_20041101_20201231_00613.txt\n",
      "Extract product file: produkt_rr_stunde_20040601_20201231_00617.txt\n",
      "Extract product file: produkt_rr_stunde_20050101_20201231_00644.txt\n",
      "Extract product file: produkt_rr_stunde_20041101_20201231_00796.txt\n",
      "Extract product file: produkt_rr_stunde_20050801_20201231_00871.txt\n",
      "Extract product file: produkt_rr_stunde_20061001_20201231_00902.txt\n",
      "Extract product file: produkt_rr_stunde_20041001_20201231_00934.txt\n",
      "Extract product file: produkt_rr_stunde_20050201_20201231_00989.txt\n",
      "Extract product file: produkt_rr_stunde_20060801_20201231_01024.txt\n",
      "Extract product file: produkt_rr_stunde_20041001_20201231_01046.txt\n",
      "Extract product file: produkt_rr_stunde_19950901_20201231_01078.txt\n",
      "Extract product file: produkt_rr_stunde_20061201_20201231_01241.txt\n",
      "Extract product file: produkt_rr_stunde_20150801_20201231_01246.txt\n",
      "Extract product file: produkt_rr_stunde_20040601_20201231_01300.txt\n",
      "Extract product file: produkt_rr_stunde_19950901_20201231_01303.txt\n",
      "Extract product file: produkt_rr_stunde_20040801_20201231_01327.txt\n",
      "Extract product file: produkt_rr_stunde_20030701_20201231_01590.txt\n",
      "Extract product file: produkt_rr_stunde_20121001_20201231_01595.txt\n",
      "Extract product file: produkt_rr_stunde_19950901_20201231_01766.txt\n",
      "Extract product file: produkt_rr_stunde_20060601_20201231_02027.txt\n",
      "Extract product file: produkt_rr_stunde_20030105_20201231_02110.txt\n",
      "Extract product file: produkt_rr_stunde_20050601_20201231_02254.txt\n",
      "Extract product file: produkt_rr_stunde_20041201_20201231_02473.txt\n",
      "Extract product file: produkt_rr_stunde_19951012_20201231_02483.txt\n",
      "Extract product file: produkt_rr_stunde_20040801_20201231_02497.txt\n",
      "Extract product file: produkt_rr_stunde_20040701_20201231_02629.txt\n",
      "Extract product file: produkt_rr_stunde_19950901_20201231_02667.txt\n",
      "Extract product file: produkt_rr_stunde_20061201_20201231_02810.txt\n",
      "Extract product file: produkt_rr_stunde_20061001_20201231_02947.txt\n",
      "Extract product file: produkt_rr_stunde_20081201_20201231_02968.txt\n",
      "Extract product file: produkt_rr_stunde_20040701_20201231_02999.txt\n",
      "Extract product file: produkt_rr_stunde_19950901_20201231_03028.txt\n",
      "Extract product file: produkt_rr_stunde_20040701_20201231_03031.txt\n",
      "Extract product file: produkt_rr_stunde_20071201_20201231_03081.txt\n",
      "Extract product file: produkt_rr_stunde_19950901_20201231_03098.txt\n",
      "Extract product file: produkt_rr_stunde_20070601_20201231_03215.txt\n",
      "Extract product file: produkt_rr_stunde_20050701_20201231_03321.txt\n",
      "Extract product file: produkt_rr_stunde_20060901_20201231_03339.txt\n",
      "Extract product file: produkt_rr_stunde_20060701_20201231_03499.txt\n",
      "Extract product file: produkt_rr_stunde_20041101_20201231_03540.txt\n",
      "Extract product file: produkt_rr_stunde_20040601_20201231_03591.txt\n",
      "Extract product file: produkt_rr_stunde_20041201_20201231_03795.txt\n",
      "Extract product file: produkt_rr_stunde_20040701_20201231_03913.txt\n",
      "Extract product file: produkt_rr_stunde_20030701_20201231_04063.txt\n",
      "Extract product file: produkt_rr_stunde_20050101_20201231_04127.txt\n",
      "Extract product file: produkt_rr_stunde_20051201_20201231_04150.txt\n",
      "Extract product file: produkt_rr_stunde_20040801_20201231_04313.txt\n",
      "Extract product file: produkt_rr_stunde_20041001_20201231_04368.txt\n",
      "Extract product file: produkt_rr_stunde_19950901_20201231_04371.txt\n",
      "Extract product file: produkt_rr_stunde_20040801_20201231_04400.txt\n",
      "Extract product file: produkt_rr_stunde_20060801_20201231_04488.txt\n",
      "Extract product file: produkt_rr_stunde_20041201_20201231_04741.txt\n",
      "Extract product file: produkt_rr_stunde_20050601_20201231_04849.txt\n",
      "Extract product file: produkt_rr_stunde_20041201_20201231_05064.txt\n",
      "Extract product file: produkt_rr_stunde_19950901_20201231_05347.txt\n",
      "Extract product file: produkt_rr_stunde_20070701_20201231_05360.txt\n",
      "Extract product file: produkt_rr_stunde_20030910_20201231_05480.txt\n",
      "Extract product file: produkt_rr_stunde_20050901_20201231_05513.txt\n",
      "Extract product file: produkt_rr_stunde_20041201_20201231_05619.txt\n",
      "Extract product file: produkt_rr_stunde_20041101_20201231_05699.txt\n",
      "Extract product file: produkt_rr_stunde_20060901_20201231_05717.txt\n",
      "Extract product file: produkt_rr_stunde_20050501_20201231_05733.txt\n",
      "Extract product file: produkt_rr_stunde_20001013_20201231_06197.txt\n",
      "Extract product file: produkt_rr_stunde_20040601_20201231_06264.txt\n",
      "Extract product file: produkt_rr_stunde_20041201_20201231_06313.txt\n",
      "Extract product file: produkt_rr_stunde_20040801_20201231_06337.txt\n",
      "Extract product file: produkt_rr_stunde_20060901_20201231_07106.txt\n",
      "Extract product file: produkt_rr_stunde_20051001_20201231_07330.txt\n",
      "Extract product file: produkt_rr_stunde_20060601_20201231_07344.txt\n",
      "Extract product file: produkt_rr_stunde_20060301_20201231_07374.txt\n",
      "Extract product file: produkt_rr_stunde_20060701_20201231_07378.txt\n",
      "Extract product file: produkt_rr_stunde_20070901_20201231_13669.txt\n",
      "Extract product file: produkt_rr_stunde_20070601_20201231_13670.txt\n",
      "Extract product file: produkt_rr_stunde_20071201_20201231_13671.txt\n",
      "Extract product file: produkt_rr_stunde_20071201_20201231_13696.txt\n",
      "Extract product file: produkt_rr_stunde_20080501_20201231_13700.txt\n",
      "Extract product file: produkt_rr_stunde_20071101_20201231_13713.txt\n",
      "Extract product file: produkt_rr_stunde_20110401_20201231_15000.txt\n",
      "create index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.LegacyCursorResult at 0x1a7ca0ff0d0>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "first = True\n",
    "\n",
    "dtypes = {\"station_id\": Numeric(6,0), \"val\" : REAL}\n",
    "\n",
    "#for elt in local_zip_list[0:1]:\n",
    "for elt in local_zip_list:\n",
    "    ffname = local_ftp_ts_dir + elt\n",
    "    #print(\"Zip archive: \" + ffname)\n",
    "    with ZipFile(ffname) as myzip:\n",
    "        # read the time series data from the file starting with \"produkt\"\n",
    "        prodfilename = [elt for elt in myzip.namelist() if elt.split(\"_\")[0]==\"produkt\"][0] \n",
    "        print(\"Extract product file: %s\" % prodfilename)\n",
    "        # print()\n",
    "        with myzip.open(prodfilename) as myfile:\n",
    "            dftmp = prec_ts_to_df(myfile)[[\"stations_id\",\"r1\"]]\n",
    "            # df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)\n",
    "            dftmp.rename(columns={'stations_id': 'station_id', 'r1': 'val', 'mess_datum': 'ts'}, inplace = True)\n",
    "            dftmp.rename_axis('ts', inplace = True)\n",
    "            # dftmp.to_csv(f, header=f.tell()==0)\n",
    "            if (first):\n",
    "                first = False\n",
    "                # dftmp.to_csv(csvfname, mode = \"w\", header = False)\n",
    "                dftmp.to_sql(name=\"prec\", schema=\"dwd\", if_exists = \"replace\", index = [\"ts\"], index_label=True, con=engine, dtype=dtypes)\n",
    "            else:\n",
    "                # dftmp.to_csv(csvfname, mode = \"a\", header = False)\n",
    "                dftmp.to_sql(name=\"prec\", schema=\"dwd\", if_exists = \"append\",  index = [\"ts\"], index_label=True, con=engine, dtype=dtypes)\n",
    "\n",
    "# After insert completed: ceate index\n",
    "print(\"create index\")\n",
    "engine.execute(\"ALTER TABLE dwd.prec ADD PRIMARY KEY (ts, station_id)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create View joining static station info with the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.LegacyCursorResult at 0x1a7c5d38c40>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute( \\\n",
    "\"\"\"\n",
    "CREATE OR REPLACE VIEW dwd.v_stations_prec \n",
    "as (select t1.station_id, t2.ts, t2.val, t1.geometry \n",
    "from dwd.stations t1, dwd.prec t2 \n",
    "where t2.ts between '2021-07-14T00:00:00UTC' and '2021-07-15T00:00:00UTC'\n",
    "and t1.station_id = t2.station_id)\n",
    "\"\"\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some checks ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql postgresql://geo_master:xxxxxx@localhost/geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql select max(ts) from dwd.v_stations_prec \n",
    "#select * from dwd.v_stations_prec where ts between '2022-02-15 23:00:00+01:00' and '2022-02-16 00:00:00+01:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql select count(*) from dwd.v_stations_prec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which Python modules are loaded?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "def imports():\n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            yield val.__name__\n",
    "list(imports())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: conda-script.py [-h] [-V] command ...\n",
      "conda-script.py: error: unrecognized arguments: pyproj\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge pyproj"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
